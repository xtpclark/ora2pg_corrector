= Ora2Pg AI Corrector
:toc:
:toclevels: 3
:source-highlighter: highlight.js
:icons: font

A smart web application to accelerate database migrations to PostgreSQL by providing AI-powered SQL conversion, intelligent validation, and integrated Ora2Pg workflow management.

== Overview

Migrating databases to PostgreSQL often involves translating SQL from various source dialects (Oracle, MySQL, SQL Server) and extensive testing to ensure compatibility. The Ora2Pg AI Corrector streamlines this process by combining:

* **AI-powered multi-dialect SQL conversion** - Convert SQL from any major database to PostgreSQL
* **Integrated Ora2Pg workflow** - Run schema discovery and exports directly from the UI
* **Intelligent validation engine** - Self-healing validation that automatically resolves dependencies
* **Multi-client workspace** - Manage multiple migration projects with isolated configurations

== Key Features

=== Multi-Dialect SQL Conversion

Convert SQL from multiple source databases to PostgreSQL:

* **Oracle** → PostgreSQL (primary focus)
* **MySQL** → PostgreSQL
* **SQL Server** → PostgreSQL
* **PostgreSQL** → PostgreSQL (optimization)
* **Generic SQL** → PostgreSQL

The AI automatically handles dialect-specific conversions:

* Data types (VARCHAR2 → VARCHAR, NUMBER → NUMERIC)
* Functions (NVL → COALESCE, DECODE → CASE)
* Syntax (DUAL removal, SYSDATE → CURRENT_TIMESTAMP)
* Sequences and identity columns
* Outer join operators

=== Dual Workflow: Migration + Workspace

==== Migration Pane (Ora2Pg Integration)

Designed for comprehensive Oracle migrations:

* **Assessment Reports**: Generate Ora2Pg migration cost and complexity reports
* **Object Discovery**: Browse and select specific schema objects (tables, views, functions)
* **Batch Export**: Export selected objects with Ora2Pg
* **Session Management**: Track export sessions and generated files
* **Original DDL Download**: Retrieve Oracle DDL with optional pretty formatting

==== Workspace Pane (Ad-hoc Conversion)

Designed for quick conversions and testing:

* **Load or paste any SQL** from files or clipboard
* **Select source dialect** from dropdown
* **Convert with AI** to PostgreSQL
* **Validate** against live database
* **No file requirements** - works with arbitrary SQL snippets

=== Intelligent Validation Engine

The validation system uses a multi-layered "self-healing" approach:

==== Proactive Schema Generation

For DML queries (SELECT, INSERT, UPDATE):

. Parses SQL to extract all required table names
. Checks validation database for existing tables
. Makes a **single consolidated AI call** to generate DDL for all missing tables
. Creates schema before validation attempt

This eliminates the need for incremental table creation and dramatically reduces API calls.

==== Reactive Self-Healing

When errors occur during validation:

**Dependency Resolution**::
Catches `relation "..." does not exist` errors and generates the specific missing object DDL on-the-fly.

**Query Correction**::
Catches syntax or semantic errors, sends the query + error to AI for correction, then retries with the fixed SQL.

**Retry Logic**::
Up to 5 automatic retry attempts with AI assistance before failing.

==== User Controls

Two checkboxes provide fine-grained control:

[cols="1,3"]
|===
|Control |Description

|**Auto-create Tables** _(checked by default)_
|Enables proactive and reactive DDL generation. When disabled, validation fails immediately if tables are missing.

|**Clean Slate** _(unchecked by default)_
|When checked, drops all tables referenced in the query before validation begins, ensuring a fresh environment.
|===

==== Validation Workflows

The combination of controls enables four distinct workflows:

[cols="2,3,3", options="header"]
|===
|Mode |Use Case |Behavior

|**Blank Canvas** +
(Clean Slate ✓, Auto-create ✓)
|Starting fresh projects
|Wipes DB, then AI creates all needed tables

|**Iterative Development** +
(Clean Slate ✗, Auto-create ✓)
|Building incrementally
|Keeps existing tables, creates new ones as needed

|**Load and Test** +
(Clean Slate ✓, Auto-create ✗)
|Testing with pre-loaded schema
|Wipes tables, fails if missing (won't create)

|**Pure Validator** +
(Clean Slate ✗, Auto-create ✗)
|Testing against "golden" schema
|Validates only, never modifies database
|===

=== Multi-Client Management

* **Client Isolation**: Each client has separate configurations, sessions, and files
* **Add/Rename/Delete**: Full client lifecycle management with confirmation dialogs
* **Active Configuration Display**: Sidebar shows current client's key settings
* **Audit Logging**: Track all actions per client

=== Modern UI Features

* **Light/Dark Theme**: Toggle between themes with persistent preference
* **Responsive Design**: Clean, modern interface with minimal visual clutter
* **CodeMirror Editors**: Syntax-highlighted SQL editors with theme support
* **Inline Validation Results**: Success/error messages appear directly below validation controls

== Getting Started

=== Prerequisites

* Docker
* Docker Compose

=== Installation

. Clone the repository:
+
[source,bash]
----
git clone <your-repository-url>
cd ora2pg-ai-corrector
----

. Create environment configuration:
+
[source,bash]
----
cp .env.example .env
----

. Edit `.env` with your settings:
+
[source,bash]
----
# Application database (SQLite or PostgreSQL)
DB_BACKEND=sqlite
# PG_DSN_CONFIG=postgresql://user:pass@host:port/app_db

# Validation database
VALIDATION_PG_DSN=postgresql://user:pass@host:port/validation_db

# Security
APP_SECRET_KEY=your_super_secret_key
APP_ENCRYPTION_KEY=your_32_byte_fernet_encryption_key
----

. Build and run:
+
[source,bash]
----
docker-compose up --build
----

. Access the application at `http://localhost:8000`

=== Quick Start: Workspace Workflow

. **Create a client**: Click the + icon next to "Client" in the sidebar
. **Configure settings**: Go to Settings tab and add:
   * AI Provider credentials (OpenAI, Anthropic, Google)
   * Validation PostgreSQL DSN (optional, for validation)
. **Go to Workspace tab**
. **Select source dialect** from dropdown (Oracle, MySQL, etc.)
. **Paste SQL** in the Source editor
. **Click Convert**: AI translates to PostgreSQL
. **Click Validate**: Test against live database (if configured)

=== Quick Start: Migration Workflow

. **Create a client** and configure settings including:
   * Oracle connection details (DSN, user, password, schema)
   * AI Provider credentials
. **Go to Migration tab**
. **Generate Assessment Report**: Get migration complexity analysis
. **Discover Objects**: Browse Oracle schema objects
. **Select objects** to export
. **Export Selected**: Run Ora2Pg with your selections
. **Click files** in session history to load into Workspace for further refinement

== Ora2Pg vs AI: When Each Is Used

Understanding when Ora2Pg handles conversions natively versus when AI intervention is needed helps predict token costs and migration complexity.

=== Ora2Pg Native Handling (No AI Required)

Ora2Pg v24.3 handles most Oracle-to-PostgreSQL conversions automatically with high accuracy:

==== DDL Structures

[cols="1,2", options="header"]
|===
|Category |Examples

|**Tables & Columns**
|CREATE TABLE, column definitions, NOT NULL constraints

|**Data Types**
|VARCHAR2→VARCHAR, NUMBER→NUMERIC/INTEGER, DATE→TIMESTAMP, CLOB→TEXT, BLOB→BYTEA, NCLOB→TEXT, BFILE→BYTEA, XMLTYPE→XML

|**Constraints**
|PRIMARY KEY, FOREIGN KEY, UNIQUE, CHECK constraints

|**Indexes**
|B-tree indexes, unique indexes (converted automatically)

|**Sequences**
|Oracle sequences to PostgreSQL sequences

|**Comments**
|Table and column comments preserved
|===

==== Simple PL/SQL

[cols="1,1", options="header"]
|===
|Oracle |PostgreSQL (Ora2Pg)

|`NVL(x, y)`
|`COALESCE(x, y)`

|`DECODE(x, a, b, c, d, e)`
|`CASE WHEN x=a THEN b WHEN x=c THEN d ELSE e END`

|`SYSDATE`
|`CURRENT_TIMESTAMP` or `statement_timestamp()`

|`ROWNUM <= n`
|`LIMIT n`

|`SYS_REFCURSOR`
|`REFCURSOR`

|`DBMS_OUTPUT.PUT_LINE()`
|`RAISE NOTICE`

|`VARCHAR2(n)`
|`VARCHAR(n)`

|`NUMBER`
|`BIGINT` or `NUMERIC`
|===

==== Migration Results (Real-World Example)

Testing against the Adempiere ERP schema (464 tables, 138 indexes, 3 sequences):

[cols="1,1,1", options="header"]
|===
|Metric |Value |AI Tokens

|Total Objects
|604
|0

|Validated Successfully
|604 (100%)
|0

|AI Corrections Needed
|0
|0
|===

*Conclusion*: For DDL-only migrations, Ora2Pg produces PostgreSQL-compatible output that validates directly without AI intervention.

=== AI Required: Complex PL/SQL Patterns

AI conversion is needed for Oracle-specific PL/SQL constructs that have no direct PostgreSQL equivalent:

==== Collection Types & Bulk Operations

[cols="1,1", options="header"]
|===
|Oracle (Requires AI) |PostgreSQL (AI Output)

|`TYPE tab IS TABLE OF NUMBER`
|`NUMERIC[]` (array type)

|`BULK COLLECT INTO collection`
|`SELECT ARRAY_AGG(col) INTO array_var`

|`FORALL i IN 1..collection.COUNT`
|`UPDATE ... WHERE col = ANY(array_var)`

|`collection(i)` (index access)
|Array operations with `ANY()`, `ALL()`, or `unnest()`
|===

==== Package Conversion

Oracle packages must be decomposed into separate PostgreSQL functions:

[source,sql]
----
-- Oracle Package
CREATE PACKAGE my_pkg AS
    c_constant CONSTANT NUMBER := 100;
    TYPE rec_type IS RECORD (...);
    PROCEDURE do_something(p_in NUMBER);
    FUNCTION get_value RETURN NUMBER;
END my_pkg;

-- PostgreSQL (AI converts to)
-- Constants become session variables or config table
-- Types become composite types
-- Each procedure/function becomes standalone
CREATE FUNCTION my_pkg_do_something(p_in NUMERIC) RETURNS VOID ...
CREATE FUNCTION my_pkg_get_value() RETURNS NUMERIC ...
----

==== Other AI-Required Patterns

[cols="1,2", options="header"]
|===
|Pattern |Why AI Is Needed

|**Autonomous Transactions**
|`PRAGMA AUTONOMOUS_TRANSACTION` → Requires `dblink` or separate connection

|**MERGE Statement**
|Complex MERGE → `INSERT ... ON CONFLICT` with conditions

|**Hierarchical Queries**
|`CONNECT BY` → Recursive CTEs (WITH RECURSIVE)

|**Object Types with Methods**
|Oracle object methods → PostgreSQL functions

|**Associative Arrays**
|`INDEX BY VARCHAR2` → `hstore` or JSONB

|**Pipelined Functions**
|`PIPELINED RETURN` → `RETURNS SETOF` with `RETURN NEXT`

|**Dynamic SQL with EXECUTE IMMEDIATE**
|May need adjustment for parameter binding style
|===

=== Token Usage Example

Converting a procedure with `BULK COLLECT` and `FORALL`:

[cols="1,1", options="header"]
|===
|Metric |Value

|Input Tokens
|~1,000

|Output Tokens
|~250

|Total Tokens
|~1,250

|Estimated Cost (Claude Sonnet)
|~$0.005
|===

=== Optimization Strategy

. **Run DDL migrations first** - Tables, indexes, sequences typically need no AI
. **Test procedures individually** - Use Workspace to convert complex PL/SQL
. **Batch similar objects** - Group procedures that use similar patterns
. **Review Ora2Pg output** - Sometimes only minor manual fixes are needed

=== Validation Note

IMPORTANT: The application uses `SET check_function_bodies = false` during DDL creation to allow dependent objects to be created in any order. This means procedures are accepted syntactically but may fail at runtime if they contain unconverted Oracle syntax. Always test procedures by executing them after migration.

== Configuration

=== Ora2Pg Options (Dynamic Configuration)

The application dynamically supports **any Ora2Pg configuration option** without code changes. The system automatically:

. Parses `ora2pg_config/default.cfg` on startup
. Loads all options into the database
. Generates UI form fields based on option metadata
. Validates and applies options during Ora2Pg execution

==== Adding New Options

To support additional Ora2Pg options:

. Edit `ora2pg_config/default.cfg` and add the option using Ora2Pg's format:
+
[source,cfg]
----
# Comment describing the option
OPTION_NAME    default_value

# For dropdowns, use pipe-separated values
TYPE    TABLE|VIEW|SEQUENCE|FUNCTION
----

. Restart the application (or reinitialize the database)
. The option automatically appears in Settings → Ora2Pg Settings
. Users can configure it per-client

==== Option Types

The parser automatically detects:

**Text Fields**:: Single-line values (DSN, schema names, paths)
**Checkboxes**:: Boolean options (0/1, true/false)
**Dropdowns**:: Options with pipe-separated allowed values
**Passwords**:: Options ending in `_PWD` or `_PASSWORD`

==== Example: Adding Custom Option

To add support for Ora2Pg's `PG_SUPPORTS_INOUT` option:

[source,cfg]
----
# Edit ora2pg_config/default.cfg
# Set to 1 if PostgreSQL >= 8.1
PG_SUPPORTS_INOUT    1
----

After restart, this appears in the UI as a checkbox under Ora2Pg Settings.

==== Reference Documentation

For the complete list of Ora2Pg options and their meanings, see:
https://ora2pg.darold.net/documentation.html

=== AI Providers

Edit `ai_config/ai_providers.json` to add or modify AI providers:

[source,json]
----
{
  "name": "OpenAI",
  "api_endpoint": "https://api.openai.com/v1",
  "default_model": "gpt-4"
}
----

=== Client-Specific Settings

Each client can configure:

* **Oracle Connection**: DSN, username, password, schema
* **AI Settings**: Provider, model, endpoint, API key, temperature, max tokens
* **Ora2Pg Options**: Any option from `default.cfg` - automatically available
* **Validation Database**: PostgreSQL DSN for testing converted SQL
=== AI Providers

Edit `ai_config/ai_providers.json` to add or modify AI providers:

[source,json]
----
{
  "name": "OpenAI",
  "api_endpoint": "https://api.openai.com/v1",
  "default_model": "gpt-4"
}
----

=== Ora2Pg Options

The `ora2pg_config/default.cfg` file populates available Ora2Pg configuration options in the UI. Options are automatically loaded into the database on first run.

=== Client-Specific Settings

Each client can configure:

* **Oracle Connection**: DSN, username, password, schema
* **AI Settings**: Provider, model, endpoint, API key, temperature, max tokens
* **Ora2Pg Options**: Export types, output directory, file-per-table, etc.
* **Validation Database**: PostgreSQL DSN for testing converted SQL

== Architecture

=== Components

**Frontend**::
* Vanilla JavaScript with ES6 modules
* CodeMirror 6 for SQL editing
* Tailwind CSS for styling
* Theme-aware with localStorage persistence

**Backend**::
* Python Flask application
* SQLite or PostgreSQL for application data
* Ora2Pg integration via subprocess
* SQL*Plus for Oracle DDL extraction

**AI Integration**::
* Multi-provider support (OpenAI, Anthropic, Google)
* Streaming and non-streaming modes
* Token usage tracking
* Configurable temperature and max tokens

=== Security

* **Encryption**: API keys and passwords encrypted with Fernet
* **Validation**: SQL injection protection for Oracle identifiers
* **Isolation**: Client data fully separated
* **Audit Logging**: All actions tracked per client

== Testing Examples

=== Example 1: Oracle to PostgreSQL DDL

.Oracle Input:
[source,sql]
----
CREATE TABLE employees (
    emp_id NUMBER PRIMARY KEY,
    emp_name VARCHAR2(100),
    hire_date DATE DEFAULT SYSDATE,
    salary NUMBER(10,2)
);
----

.PostgreSQL Output:
[source,sql]
----
CREATE TABLE employees (
    emp_id INTEGER PRIMARY KEY,
    emp_name VARCHAR(100),
    hire_date DATE DEFAULT CURRENT_TIMESTAMP,
    salary NUMERIC(10,2)
);
----

=== Example 2: Self-Healing Validation

.Query with Missing Tables:
[source,sql]
----
SELECT e.emp_name, d.dept_name
FROM employees e
JOIN departments d ON e.dept_id = d.dept_id;
----

**Validation Process**:

. Proactive engine detects `employees` and `departments` are missing
. Single AI call generates both table DDLs
. Tables created automatically
. Query validated successfully

=== Example 3: Syntax Error Correction

.Query with Error:
[source,sql]
----
SELECT * FROM products ORDER BY price LIMIT DESC 10;
----

**Validation Process**:

. Query fails with syntax error at "DESC"
. AI receives error message and query
. AI returns corrected query: `ORDER BY price DESC LIMIT 10`
. Retry succeeds

== Troubleshooting

=== Validation Fails with "Database not configured"

Ensure the client has a `validation_pg_dsn` set in Settings → Validation Database.

=== AI Conversion Returns Unchanged SQL

Check that:

* AI provider credentials are correct
* Model supports the task (use GPT-4 or Claude for best results)
* Source dialect is selected correctly

=== Ora2Pg Commands Fail

Verify in Settings:

* Oracle DSN is correct (format: `dbi:Oracle:host=X;port=Y;service_name=Z`)
* Oracle credentials are valid
* Schema name is correct and accessible

=== Theme Not Persisting

The theme preference is stored in browser localStorage. Clear browser cache if experiencing issues.

== Recent Enhancements

=== DDL Caching

AI-generated DDL is cached to reduce API calls and provide an audit trail.

==== How It Works

When validation encounters a missing table, the system:

. Checks the database cache for existing DDL
. If found, reuses cached DDL (increments hit count)
. If not found, generates DDL via AI and stores it in both:
   * Database (for automatic reuse)
   * File system (for human review)

==== File Structure

[source]
----
/app/project_data/{client_id}/{session_id}/
├── output_table.sql           # Ora2Pg export
├── output_view.sql            # Ora2Pg export
└── ai_generated_ddl/          # AI-generated DDL folder
    ├── employees.sql          # CREATE TABLE employees...
    ├── departments.sql        # CREATE TABLE departments...
    └── _manifest.json         # Index with metadata
----

==== API Endpoints

[cols="1,2,3", options="header"]
|===
|Method |Endpoint |Description

|GET |`/api/client/{id}/ddl_cache/stats` |View cache statistics and hit counts
|DELETE |`/api/client/{id}/ddl_cache` |Clear all cached DDL for a client
|GET |`/api/session/{id}/generated_ddl` |List AI-generated DDL files
|GET |`/api/session/{id}/generated_ddl/{name}` |Get specific DDL file content
|===

==== Example: Cache Stats Response

[source,json]
----
{
  "client_id": 1,
  "total_entries": 3,
  "total_hits": 12,
  "entries": [
    {
      "object_name": "employees",
      "object_type": "TABLE",
      "hit_count": 8,
      "ai_provider": "Anthropic Claude",
      "ai_model": "claude-sonnet-4-20250514",
      "created_at": "2025-12-11 04:17:06"
    }
  ]
}
----

=== Migration Reports

Generate AsciiDoc reports documenting migration results for stakeholders.

==== Report Contents

* **Executive Summary**: Status, success rate, duration
* **Object Summary**: Counts by type (TABLE, VIEW, PROCEDURE) with success rates
* **File Details**: Each file's status, size, and any errors
* **Error Details**: Full error messages with context
* **Metadata**: AI provider, model, Ora2Pg version, timestamps

==== API Endpoints

[cols="1,2,3", options="header"]
|===
|Method |Endpoint |Description

|GET |`/api/session/{id}/report` |Generate AsciiDoc report for session
|GET |`/api/session/{id}/report/download` |Download as `.adoc` file
|GET |`/api/client/{id}/migration_report` |Report for client's latest migration
|===

==== Example: Report Structure

[source,asciidoc]
----
= Migration Report: HR_Migration_Test
:toc:

== Executive Summary
[cols="1,2"]
|===
|Status |Completed
|Success Rate |95% (19/20 files)
|Duration |2 minutes 34 seconds
|===

== Object Summary
[cols="1,1,1,1"]
|===
|Type |Total |Success |Failed

|TABLE |15 |15 |0
|VIEW |3 |2 |1
|PROCEDURE |2 |2 |0
|===
----

=== Rollback Scripts

Automatic generation of DROP statements for safe migration reversal.

==== How It Works

After successful validation, the system:

. Parses all validated DDL to extract object names and types
. Sorts objects in reverse dependency order (safe drop order)
. Generates a complete rollback script with:
   * Header with session metadata and warnings
   * BEGIN/COMMIT transaction wrapper
   * DROP IF EXISTS ... CASCADE for each object

==== Drop Order (Reverse Dependency)

Objects are dropped in this order to avoid dependency errors:

. TRIGGER
. PROCEDURE
. FUNCTION
. MATERIALIZED VIEW
. VIEW
. INDEX
. TABLE
. SEQUENCE
. TYPE

==== API Endpoints

[cols="1,2,3", options="header"]
|===
|Method |Endpoint |Description

|GET |`/api/session/{id}/rollback` |Get rollback script content
|GET |`/api/session/{id}/rollback/preview` |Preview objects that would be dropped
|GET |`/api/session/{id}/rollback/download` |Download as `.sql` file
|POST |`/api/session/{id}/rollback/execute` |Execute rollback (requires `confirm: true`)
|===

==== Example: Rollback Preview Response

[source,json]
----
{
  "session_id": 25,
  "total_objects": 21,
  "warning": "This script will DROP 21 objects. Review carefully before executing.",
  "objects_to_drop": [
    {"type": "PROCEDURE", "name": "add_job_history", "drop_statement": "DROP PROCEDURE IF EXISTS..."},
    {"type": "VIEW", "name": "emp_details_view", "drop_statement": "DROP VIEW IF EXISTS..."},
    {"type": "TABLE", "name": "employees", "drop_statement": "DROP TABLE IF EXISTS..."}
  ]
}
----

==== Example: Rollback Script

[source,sql]
----
-- =============================================================================
-- ROLLBACK SCRIPT
-- Session ID: 25 | Client: HR_Migration_Test
-- Generated: 2025-12-11 06:59:06
-- WARNING: This script will DROP all objects created by this migration.
-- =============================================================================

BEGIN;

-- 1. Procedures
DROP PROCEDURE IF EXISTS "add_job_history" CASCADE;

-- 2. Views
DROP VIEW IF EXISTS "emp_details_view" CASCADE;

-- 3. Tables
DROP TABLE IF EXISTS "employees" CASCADE;
DROP TABLE IF EXISTS "departments" CASCADE;

COMMIT;
----

=== Per-Object Tracking

Track validation status for individual database objects within a migration session.

==== Object States

[cols="1,3", options="header"]
|===
|Status |Description

|`pending` |Object discovered, not yet validated
|`validated` |Successfully validated against PostgreSQL
|`failed` |Validation failed (error message stored)
|===

==== API Endpoints

[cols="1,2,3", options="header"]
|===
|Method |Endpoint |Description

|GET |`/api/session/{id}/objects` |List all objects with optional filters
|GET |`/api/session/{id}/objects/summary` |Lightweight summary counts
|GET |`/api/object/{id}` |Get detailed info for specific object
|GET |`/api/client/{id}/objects/summary` |Aggregate summary across all sessions
|===

==== Query Parameters

* `?type=TABLE` - Filter by object type
* `?status=validated` - Filter by status

==== Example: Objects Summary Response

[source,json]
----
{
  "session_id": 25,
  "totals": {"total": 21, "validated": 19, "failed": 2, "pending": 0},
  "by_type": {
    "TABLE": {"total": 7, "validated": 7, "failed": 0},
    "VIEW": {"total": 1, "validated": 0, "failed": 1},
    "INDEX": {"total": 11, "validated": 10, "failed": 1},
    "PROCEDURE": {"total": 2, "validated": 2, "failed": 0}
  }
}
----

== Future Enhancements

* **Batch Processing**: Process entire directories of SQL files with summary reports
* **Migration Progress Tracking**: Visual progress indicators for long-running operations
* **Export Formats**: Support for additional output formats beyond SQL
* **Collaborative Features**: Multi-user access with role-based permissions

== Contributing

Contributions are welcome! Please ensure:

* Code follows existing style patterns
* New features include appropriate error handling
* UI changes support both light and dark themes
* Database changes include migration scripts

== License

[Specify your license here]

== Support

For issues, questions, or feature requests, please open an issue in the repository.
